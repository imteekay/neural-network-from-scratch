{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2994100,"sourceType":"datasetVersion","datasetId":1834623}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom xgboost import XGBRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:04.337645Z","iopub.execute_input":"2025-02-02T12:57:04.338072Z","iopub.status.idle":"2025-02-02T12:57:04.342780Z","shell.execute_reply.started":"2025-02-02T12:57:04.338035Z","shell.execute_reply":"2025-02-02T12:57:04.341688Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/uber-fares-dataset/uber.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:04.356417Z","iopub.execute_input":"2025-02-02T12:57:04.356719Z","iopub.status.idle":"2025-02-02T12:57:04.813246Z","shell.execute_reply.started":"2025-02-02T12:57:04.356694Z","shell.execute_reply":"2025-02-02T12:57:04.812282Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"        Unnamed: 0                            key  fare_amount  \\\n0         24238194    2015-05-07 19:52:06.0000003          7.5   \n1         27835199    2009-07-17 20:04:56.0000002          7.7   \n2         44984355   2009-08-24 21:45:00.00000061         12.9   \n3         25894730    2009-06-26 08:22:21.0000001          5.3   \n4         17610152  2014-08-28 17:47:00.000000188         16.0   \n...            ...                            ...          ...   \n199995    42598914   2012-10-28 10:49:00.00000053          3.0   \n199996    16382965    2014-03-14 01:09:00.0000008          7.5   \n199997    27804658   2009-06-29 00:42:00.00000078         30.9   \n199998    20259894    2015-05-20 14:56:25.0000004         14.5   \n199999    11951496   2010-05-15 04:08:00.00000076         14.1   \n\n                pickup_datetime  pickup_longitude  pickup_latitude  \\\n0       2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n1       2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n2       2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n3       2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n4       2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n...                         ...               ...              ...   \n199995  2012-10-28 10:49:00 UTC        -73.987042        40.739367   \n199996  2014-03-14 01:09:00 UTC        -73.984722        40.736837   \n199997  2009-06-29 00:42:00 UTC        -73.986017        40.756487   \n199998  2015-05-20 14:56:25 UTC        -73.997124        40.725452   \n199999  2010-05-15 04:08:00 UTC        -73.984395        40.720077   \n\n        dropoff_longitude  dropoff_latitude  passenger_count  \n0              -73.999512         40.723217                1  \n1              -73.994710         40.750325                1  \n2              -73.962565         40.772647                1  \n3              -73.965316         40.803349                3  \n4              -73.973082         40.761247                5  \n...                   ...               ...              ...  \n199995         -73.986525         40.740297                1  \n199996         -74.006672         40.739620                1  \n199997         -73.858957         40.692588                2  \n199998         -73.983215         40.695415                1  \n199999         -73.985508         40.768793                1  \n\n[200000 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>key</th>\n      <th>fare_amount</th>\n      <th>pickup_datetime</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24238194</td>\n      <td>2015-05-07 19:52:06.0000003</td>\n      <td>7.5</td>\n      <td>2015-05-07 19:52:06 UTC</td>\n      <td>-73.999817</td>\n      <td>40.738354</td>\n      <td>-73.999512</td>\n      <td>40.723217</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27835199</td>\n      <td>2009-07-17 20:04:56.0000002</td>\n      <td>7.7</td>\n      <td>2009-07-17 20:04:56 UTC</td>\n      <td>-73.994355</td>\n      <td>40.728225</td>\n      <td>-73.994710</td>\n      <td>40.750325</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>44984355</td>\n      <td>2009-08-24 21:45:00.00000061</td>\n      <td>12.9</td>\n      <td>2009-08-24 21:45:00 UTC</td>\n      <td>-74.005043</td>\n      <td>40.740770</td>\n      <td>-73.962565</td>\n      <td>40.772647</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25894730</td>\n      <td>2009-06-26 08:22:21.0000001</td>\n      <td>5.3</td>\n      <td>2009-06-26 08:22:21 UTC</td>\n      <td>-73.976124</td>\n      <td>40.790844</td>\n      <td>-73.965316</td>\n      <td>40.803349</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17610152</td>\n      <td>2014-08-28 17:47:00.000000188</td>\n      <td>16.0</td>\n      <td>2014-08-28 17:47:00 UTC</td>\n      <td>-73.925023</td>\n      <td>40.744085</td>\n      <td>-73.973082</td>\n      <td>40.761247</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199995</th>\n      <td>42598914</td>\n      <td>2012-10-28 10:49:00.00000053</td>\n      <td>3.0</td>\n      <td>2012-10-28 10:49:00 UTC</td>\n      <td>-73.987042</td>\n      <td>40.739367</td>\n      <td>-73.986525</td>\n      <td>40.740297</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199996</th>\n      <td>16382965</td>\n      <td>2014-03-14 01:09:00.0000008</td>\n      <td>7.5</td>\n      <td>2014-03-14 01:09:00 UTC</td>\n      <td>-73.984722</td>\n      <td>40.736837</td>\n      <td>-74.006672</td>\n      <td>40.739620</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199997</th>\n      <td>27804658</td>\n      <td>2009-06-29 00:42:00.00000078</td>\n      <td>30.9</td>\n      <td>2009-06-29 00:42:00 UTC</td>\n      <td>-73.986017</td>\n      <td>40.756487</td>\n      <td>-73.858957</td>\n      <td>40.692588</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>199998</th>\n      <td>20259894</td>\n      <td>2015-05-20 14:56:25.0000004</td>\n      <td>14.5</td>\n      <td>2015-05-20 14:56:25 UTC</td>\n      <td>-73.997124</td>\n      <td>40.725452</td>\n      <td>-73.983215</td>\n      <td>40.695415</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199999</th>\n      <td>11951496</td>\n      <td>2010-05-15 04:08:00.00000076</td>\n      <td>14.1</td>\n      <td>2010-05-15 04:08:00 UTC</td>\n      <td>-73.984395</td>\n      <td>40.720077</td>\n      <td>-73.985508</td>\n      <td>40.768793</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>200000 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"- **key**: a unique identifier for each trip\n- **fare_amount**: the cost of each trip in usd\n- **pickup_datetime**: date and time when the meter was engaged\n- **pickup_longitude**: the longitude where the meter was engaged\n- **pickup_latitude**: the latitude where the meter was engaged\n- **dropoff_longitude**: the longitude where the meter was disengaged\n- **dropoff_latitude**: the latitude where the meter was disengaged\n- **passenger_count**: the number of passengers in the vehicle (driver entered value)","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:04.814350Z","iopub.execute_input":"2025-02-02T12:57:04.814659Z","iopub.status.idle":"2025-02-02T12:57:04.820860Z","shell.execute_reply.started":"2025-02-02T12:57:04.814635Z","shell.execute_reply":"2025-02-02T12:57:04.820021Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0             int64\nkey                   object\nfare_amount          float64\npickup_datetime       object\npickup_longitude     float64\npickup_latitude      float64\ndropoff_longitude    float64\ndropoff_latitude     float64\npassenger_count        int64\ndtype: object"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"df = df.drop(columns=['Unnamed: 0', 'key'])\ndf = df.dropna()\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:04.822523Z","iopub.execute_input":"2025-02-02T12:57:04.822791Z","iopub.status.idle":"2025-02-02T12:57:04.874221Z","shell.execute_reply.started":"2025-02-02T12:57:04.822768Z","shell.execute_reply":"2025-02-02T12:57:04.873280Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"        fare_amount          pickup_datetime  pickup_longitude  \\\n0               7.5  2015-05-07 19:52:06 UTC        -73.999817   \n1               7.7  2009-07-17 20:04:56 UTC        -73.994355   \n2              12.9  2009-08-24 21:45:00 UTC        -74.005043   \n3               5.3  2009-06-26 08:22:21 UTC        -73.976124   \n4              16.0  2014-08-28 17:47:00 UTC        -73.925023   \n...             ...                      ...               ...   \n199995          3.0  2012-10-28 10:49:00 UTC        -73.987042   \n199996          7.5  2014-03-14 01:09:00 UTC        -73.984722   \n199997         30.9  2009-06-29 00:42:00 UTC        -73.986017   \n199998         14.5  2015-05-20 14:56:25 UTC        -73.997124   \n199999         14.1  2010-05-15 04:08:00 UTC        -73.984395   \n\n        pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n0             40.738354         -73.999512         40.723217                1  \n1             40.728225         -73.994710         40.750325                1  \n2             40.740770         -73.962565         40.772647                1  \n3             40.790844         -73.965316         40.803349                3  \n4             40.744085         -73.973082         40.761247                5  \n...                 ...                ...               ...              ...  \n199995        40.739367         -73.986525         40.740297                1  \n199996        40.736837         -74.006672         40.739620                1  \n199997        40.756487         -73.858957         40.692588                2  \n199998        40.725452         -73.983215         40.695415                1  \n199999        40.720077         -73.985508         40.768793                1  \n\n[199999 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fare_amount</th>\n      <th>pickup_datetime</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.5</td>\n      <td>2015-05-07 19:52:06 UTC</td>\n      <td>-73.999817</td>\n      <td>40.738354</td>\n      <td>-73.999512</td>\n      <td>40.723217</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.7</td>\n      <td>2009-07-17 20:04:56 UTC</td>\n      <td>-73.994355</td>\n      <td>40.728225</td>\n      <td>-73.994710</td>\n      <td>40.750325</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.9</td>\n      <td>2009-08-24 21:45:00 UTC</td>\n      <td>-74.005043</td>\n      <td>40.740770</td>\n      <td>-73.962565</td>\n      <td>40.772647</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.3</td>\n      <td>2009-06-26 08:22:21 UTC</td>\n      <td>-73.976124</td>\n      <td>40.790844</td>\n      <td>-73.965316</td>\n      <td>40.803349</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16.0</td>\n      <td>2014-08-28 17:47:00 UTC</td>\n      <td>-73.925023</td>\n      <td>40.744085</td>\n      <td>-73.973082</td>\n      <td>40.761247</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199995</th>\n      <td>3.0</td>\n      <td>2012-10-28 10:49:00 UTC</td>\n      <td>-73.987042</td>\n      <td>40.739367</td>\n      <td>-73.986525</td>\n      <td>40.740297</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199996</th>\n      <td>7.5</td>\n      <td>2014-03-14 01:09:00 UTC</td>\n      <td>-73.984722</td>\n      <td>40.736837</td>\n      <td>-74.006672</td>\n      <td>40.739620</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199997</th>\n      <td>30.9</td>\n      <td>2009-06-29 00:42:00 UTC</td>\n      <td>-73.986017</td>\n      <td>40.756487</td>\n      <td>-73.858957</td>\n      <td>40.692588</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>199998</th>\n      <td>14.5</td>\n      <td>2015-05-20 14:56:25 UTC</td>\n      <td>-73.997124</td>\n      <td>40.725452</td>\n      <td>-73.983215</td>\n      <td>40.695415</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199999</th>\n      <td>14.1</td>\n      <td>2010-05-15 04:08:00 UTC</td>\n      <td>-73.984395</td>\n      <td>40.720077</td>\n      <td>-73.985508</td>\n      <td>40.768793</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>199999 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:04.875731Z","iopub.execute_input":"2025-02-02T12:57:04.876123Z","iopub.status.idle":"2025-02-02T12:57:04.900705Z","shell.execute_reply.started":"2025-02-02T12:57:04.876089Z","shell.execute_reply":"2025-02-02T12:57:04.899874Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 199999 entries, 0 to 199999\nData columns (total 7 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   fare_amount        199999 non-null  float64\n 1   pickup_datetime    199999 non-null  object \n 2   pickup_longitude   199999 non-null  float64\n 3   pickup_latitude    199999 non-null  float64\n 4   dropoff_longitude  199999 non-null  float64\n 5   dropoff_latitude   199999 non-null  float64\n 6   passenger_count    199999 non-null  int64  \ndtypes: float64(5), int64(1), object(1)\nmemory usage: 12.2+ MB\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### Pickup datetime\n\nThe most important part of the pickup datetime is actually the hour it started. It can in the morning, afternoon, evening, rush time, and that influences the Uber fare.","metadata":{}},{"cell_type":"code","source":"df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], utc=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:04.901610Z","iopub.execute_input":"2025-02-02T12:57:04.902128Z","iopub.status.idle":"2025-02-02T12:57:07.473492Z","shell.execute_reply.started":"2025-02-02T12:57:04.902092Z","shell.execute_reply":"2025-02-02T12:57:07.472674Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"df['hour'] = df['pickup_datetime'].dt.hour\n\ndef get_time_of_day(hour):\n    if 7 <= hour <= 9 or 16 <= hour <= 18:\n        return 'Rush Hour'\n    elif 0 <= hour < 6:\n        return 'Early Morning'\n    elif 6 <= hour < 12:\n        return 'Morning'\n    elif 12 <= hour < 18:\n        return 'Afternoon'\n    else:\n        return 'Evening'\n\ndf['time_of_day'] = df['hour'].apply(get_time_of_day)\ndf['day_of_week'] = df['pickup_datetime'].dt.day_name()\ndf = pd.get_dummies(df, columns=['time_of_day', 'day_of_week'], prefix='', prefix_sep='', dtype='float')\ndf = df.drop(columns=['pickup_datetime'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:07.474137Z","iopub.execute_input":"2025-02-02T12:57:07.474391Z","iopub.status.idle":"2025-02-02T12:57:07.705233Z","shell.execute_reply.started":"2025-02-02T12:57:07.474369Z","shell.execute_reply":"2025-02-02T12:57:07.704286Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"        fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n0               7.5        -73.999817        40.738354         -73.999512   \n1               7.7        -73.994355        40.728225         -73.994710   \n2              12.9        -74.005043        40.740770         -73.962565   \n3               5.3        -73.976124        40.790844         -73.965316   \n4              16.0        -73.925023        40.744085         -73.973082   \n...             ...               ...              ...                ...   \n199995          3.0        -73.987042        40.739367         -73.986525   \n199996          7.5        -73.984722        40.736837         -74.006672   \n199997         30.9        -73.986017        40.756487         -73.858957   \n199998         14.5        -73.997124        40.725452         -73.983215   \n199999         14.1        -73.984395        40.720077         -73.985508   \n\n        dropoff_latitude  passenger_count  hour  Afternoon  Early Morning  \\\n0              40.723217                1    19        0.0            0.0   \n1              40.750325                1    20        0.0            0.0   \n2              40.772647                1    21        0.0            0.0   \n3              40.803349                3     8        0.0            0.0   \n4              40.761247                5    17        0.0            0.0   \n...                  ...              ...   ...        ...            ...   \n199995         40.740297                1    10        0.0            0.0   \n199996         40.739620                1     1        0.0            1.0   \n199997         40.692588                2     0        0.0            1.0   \n199998         40.695415                1    14        1.0            0.0   \n199999         40.768793                1     4        0.0            1.0   \n\n        Evening  Morning  Rush Hour  Friday  Monday  Saturday  Sunday  \\\n0           1.0      0.0        0.0     0.0     0.0       0.0     0.0   \n1           1.0      0.0        0.0     1.0     0.0       0.0     0.0   \n2           1.0      0.0        0.0     0.0     1.0       0.0     0.0   \n3           0.0      0.0        1.0     1.0     0.0       0.0     0.0   \n4           0.0      0.0        1.0     0.0     0.0       0.0     0.0   \n...         ...      ...        ...     ...     ...       ...     ...   \n199995      0.0      1.0        0.0     0.0     0.0       0.0     1.0   \n199996      0.0      0.0        0.0     1.0     0.0       0.0     0.0   \n199997      0.0      0.0        0.0     0.0     1.0       0.0     0.0   \n199998      0.0      0.0        0.0     0.0     0.0       0.0     0.0   \n199999      0.0      0.0        0.0     0.0     0.0       1.0     0.0   \n\n        Thursday  Tuesday  Wednesday  \n0            1.0      0.0        0.0  \n1            0.0      0.0        0.0  \n2            0.0      0.0        0.0  \n3            0.0      0.0        0.0  \n4            1.0      0.0        0.0  \n...          ...      ...        ...  \n199995       0.0      0.0        0.0  \n199996       0.0      0.0        0.0  \n199997       0.0      0.0        0.0  \n199998       0.0      0.0        1.0  \n199999       0.0      0.0        0.0  \n\n[199999 rows x 19 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fare_amount</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n      <th>hour</th>\n      <th>Afternoon</th>\n      <th>Early Morning</th>\n      <th>Evening</th>\n      <th>Morning</th>\n      <th>Rush Hour</th>\n      <th>Friday</th>\n      <th>Monday</th>\n      <th>Saturday</th>\n      <th>Sunday</th>\n      <th>Thursday</th>\n      <th>Tuesday</th>\n      <th>Wednesday</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.5</td>\n      <td>-73.999817</td>\n      <td>40.738354</td>\n      <td>-73.999512</td>\n      <td>40.723217</td>\n      <td>1</td>\n      <td>19</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.7</td>\n      <td>-73.994355</td>\n      <td>40.728225</td>\n      <td>-73.994710</td>\n      <td>40.750325</td>\n      <td>1</td>\n      <td>20</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.9</td>\n      <td>-74.005043</td>\n      <td>40.740770</td>\n      <td>-73.962565</td>\n      <td>40.772647</td>\n      <td>1</td>\n      <td>21</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.3</td>\n      <td>-73.976124</td>\n      <td>40.790844</td>\n      <td>-73.965316</td>\n      <td>40.803349</td>\n      <td>3</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16.0</td>\n      <td>-73.925023</td>\n      <td>40.744085</td>\n      <td>-73.973082</td>\n      <td>40.761247</td>\n      <td>5</td>\n      <td>17</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199995</th>\n      <td>3.0</td>\n      <td>-73.987042</td>\n      <td>40.739367</td>\n      <td>-73.986525</td>\n      <td>40.740297</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>199996</th>\n      <td>7.5</td>\n      <td>-73.984722</td>\n      <td>40.736837</td>\n      <td>-74.006672</td>\n      <td>40.739620</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>199997</th>\n      <td>30.9</td>\n      <td>-73.986017</td>\n      <td>40.756487</td>\n      <td>-73.858957</td>\n      <td>40.692588</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>199998</th>\n      <td>14.5</td>\n      <td>-73.997124</td>\n      <td>40.725452</td>\n      <td>-73.983215</td>\n      <td>40.695415</td>\n      <td>1</td>\n      <td>14</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>199999</th>\n      <td>14.1</td>\n      <td>-73.984395</td>\n      <td>40.720077</td>\n      <td>-73.985508</td>\n      <td>40.768793</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>199999 rows × 19 columns</p>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"training_percentage = 0.80\nexamples_size = len(df)\ntraining_size = int(examples_size * training_percentage)\ndata = np.array(df)\nnp.random.shuffle(data)\n\ntraining_data = data[:training_size].T\ntest_data = data[training_size:].T\n\nY_training = training_data[0].reshape(-1, 1)\nX_training = np.delete(training_data, 0, axis=0).T\n\nY_test = test_data[0].reshape(-1, 1)\nX_test = np.delete(test_data, 0, axis=0).T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:07.706209Z","iopub.execute_input":"2025-02-02T12:57:07.706540Z","iopub.status.idle":"2025-02-02T12:57:08.112367Z","shell.execute_reply.started":"2025-02-02T12:57:07.706509Z","shell.execute_reply":"2025-02-02T12:57:08.111342Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"X_training.shape, X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:08.113274Z","iopub.execute_input":"2025-02-02T12:57:08.113534Z","iopub.status.idle":"2025-02-02T12:57:08.119203Z","shell.execute_reply.started":"2025-02-02T12:57:08.113511Z","shell.execute_reply":"2025-02-02T12:57:08.118025Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"((159999, 18), (40000, 18))"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"Y_training.shape, Y_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:08.122448Z","iopub.execute_input":"2025-02-02T12:57:08.122737Z","iopub.status.idle":"2025-02-02T12:57:08.135623Z","shell.execute_reply.started":"2025-02-02T12:57:08.122714Z","shell.execute_reply":"2025-02-02T12:57:08.134761Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"((159999, 1), (40000, 1))"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"## Scaling Features","metadata":{}},{"cell_type":"code","source":"X_scaler = StandardScaler()\nY_scaler = StandardScaler()\n\nX_training_scaled = X_scaler.fit_transform(X_training)\nX_test_scaled = X_scaler.transform(X_test)\n\nY_training_scaled = Y_scaler.fit_transform(Y_training.reshape(-1, 1)).ravel()\nY_test_scaled = Y_scaler.transform(Y_test.reshape(-1, 1)).ravel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:08.137619Z","iopub.execute_input":"2025-02-02T12:57:08.137853Z","iopub.status.idle":"2025-02-02T12:57:08.209048Z","shell.execute_reply.started":"2025-02-02T12:57:08.137833Z","shell.execute_reply":"2025-02-02T12:57:08.208198Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"X_training_scaled.shape, Y_training_scaled.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:08.209903Z","iopub.execute_input":"2025-02-02T12:57:08.210266Z","iopub.status.idle":"2025-02-02T12:57:08.215800Z","shell.execute_reply.started":"2025-02-02T12:57:08.210232Z","shell.execute_reply":"2025-02-02T12:57:08.214953Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"((159999, 18), (159999,))"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"X_test_scaled.shape, Y_test_scaled.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:08.216763Z","iopub.execute_input":"2025-02-02T12:57:08.217124Z","iopub.status.idle":"2025-02-02T12:57:08.234300Z","shell.execute_reply.started":"2025-02-02T12:57:08.217090Z","shell.execute_reply":"2025-02-02T12:57:08.233419Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"((40000, 18), (40000,))"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"### Basic hyperparameters","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(objective='reg:squarederror', \n                     n_estimators=1000, # Number of boosting rounds\n                     learning_rate=0.1, # Step size at each iteration\n                     max_depth=10 # Maximum depth of a tree\n                    )\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:08.235204Z","iopub.execute_input":"2025-02-02T12:57:08.235478Z","iopub.status.idle":"2025-02-02T12:57:19.294713Z","shell.execute_reply.started":"2025-02-02T12:57:08.235455Z","shell.execute_reply":"2025-02-02T12:57:19.293062Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-abbbb58c88a7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_training_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_training_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             )\n\u001b[0;32m-> 1090\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1091\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":37},{"cell_type":"markdown","source":"### Without scaling","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(objective='reg:squarederror', \n                     n_estimators=1000, # Number of boosting rounds\n                     learning_rate=0.1, # Step size at each iteration\n                     max_depth=10 # Maximum depth of a tree\n                    )\n\nmodel.fit(X_training, Y_training)\nprediction = model.predict(X_test)\n\nmse = mean_squared_error(Y_test, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.295300Z","iopub.status.idle":"2025-02-02T12:57:19.295593Z","shell.execute_reply":"2025-02-02T12:57:19.295475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### New hyperparameters","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=2000,\n    learning_rate=0.05,\n    max_depth=6,  \n    subsample=0.8,  # Helps generalization\n    colsample_bytree=0.8,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=0.1,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.296563Z","iopub.status.idle":"2025-02-02T12:57:19.297017Z","shell.execute_reply":"2025-02-02T12:57:19.296807Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reg Alpha: 0.1 -> 10","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=2000,\n    learning_rate=0.05,\n    max_depth=6,  \n    subsample=0.8,  # Helps generalization\n    colsample_bytree=0.8,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.297703Z","iopub.status.idle":"2025-02-02T12:57:19.298053Z","shell.execute_reply":"2025-02-02T12:57:19.297888Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Learning Rate: 0.5 -> 0.1","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=6,  \n    subsample=0.8,  # Helps generalization\n    colsample_bytree=0.8,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.299147Z","iopub.status.idle":"2025-02-02T12:57:19.299467Z","shell.execute_reply":"2025-02-02T12:57:19.299345Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## N Estimators: 2000 -> 5000","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=5000,\n    learning_rate=0.01,\n    max_depth=6,  \n    subsample=0.8,  # Helps generalization\n    colsample_bytree=0.8,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.300236Z","iopub.status.idle":"2025-02-02T12:57:19.300582Z","shell.execute_reply":"2025-02-02T12:57:19.300424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Subsample: 0.8 -> 0.85","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=5000,\n    learning_rate=0.01,\n    max_depth=6,  \n    subsample=0.85,  # Helps generalization\n    colsample_bytree=0.8,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.301197Z","iopub.status.idle":"2025-02-02T12:57:19.301495Z","shell.execute_reply":"2025-02-02T12:57:19.301374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Subsample: 0.85 -> 0.9","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=5000,\n    learning_rate=0.01,\n    max_depth=6,  \n    subsample=0.9,  # Helps generalization\n    colsample_bytree=0.8,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.302190Z","iopub.status.idle":"2025-02-02T12:57:19.302503Z","shell.execute_reply":"2025-02-02T12:57:19.302377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Subsample: 0.9 -> 0.95","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=5000,\n    learning_rate=0.01,\n    max_depth=6,  \n    subsample=0.95,  # Helps generalization\n    colsample_bytree=0.8,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.303418Z","iopub.status.idle":"2025-02-02T12:57:19.303732Z","shell.execute_reply":"2025-02-02T12:57:19.303596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Colsample bytree: 0.8 -> 0.9","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=5000,\n    learning_rate=0.01,\n    max_depth=6,  \n    subsample=0.95,  # Helps generalization\n    colsample_bytree=0.9,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.304662Z","iopub.status.idle":"2025-02-02T12:57:19.305001Z","shell.execute_reply":"2025-02-02T12:57:19.304845Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Max Depth: 6 -> 5","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=5000,\n    learning_rate=0.01,\n    max_depth=5,  \n    subsample=0.95,  # Helps generalization\n    colsample_bytree=0.9,  # Random feature selection per tree\n    gamma=0.1,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.305691Z","iopub.status.idle":"2025-02-02T12:57:19.306071Z","shell.execute_reply":"2025-02-02T12:57:19.305878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Gamma: 0.1 -> 0.01","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=5000,\n    learning_rate=0.01,\n    max_depth=5,  \n    subsample=0.95,  # Helps generalization\n    colsample_bytree=0.9,  # Random feature selection per tree\n    gamma=0.01,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.307055Z","iopub.status.idle":"2025-02-02T12:57:19.307440Z","shell.execute_reply":"2025-02-02T12:57:19.307276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Learning Rate: 0.01 -> 0.009","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=5000,\n    learning_rate=0.009,\n    max_depth=5,  \n    subsample=0.95,  # Helps generalization\n    colsample_bytree=0.9,  # Random feature selection per tree\n    gamma=0.01,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.308143Z","iopub.status.idle":"2025-02-02T12:57:19.308429Z","shell.execute_reply":"2025-02-02T12:57:19.308315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## N Estimators: 5000 -> 6000","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=6000,\n    learning_rate=0.009,\n    max_depth=5,  \n    subsample=0.95,  # Helps generalization\n    colsample_bytree=0.9,  # Random feature selection per tree\n    gamma=0.01,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.308989Z","iopub.status.idle":"2025-02-02T12:57:19.309274Z","shell.execute_reply":"2025-02-02T12:57:19.309158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## N Estimators: 6000 -> 7000","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=7000,\n    learning_rate=0.009,\n    max_depth=5,  \n    subsample=0.95,  # Helps generalization\n    colsample_bytree=0.9,  # Random feature selection per tree\n    gamma=0.01,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.310010Z","iopub.status.idle":"2025-02-02T12:57:19.310290Z","shell.execute_reply":"2025-02-02T12:57:19.310172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=8000,\n    learning_rate=0.009,\n    max_depth=5,  \n    subsample=0.95,  # Helps generalization\n    colsample_bytree=0.9,  # Random feature selection per tree\n    gamma=0.01,  # Reduces overfitting\n    reg_alpha=10,  # L1 regularization\n    reg_lambda=0.1,  # L2 regularization\n    random_state=42\n)\n\nmodel.fit(X_training_scaled, Y_training_scaled)\nprediction = model.predict(X_test_scaled)\n\nmse = mean_squared_error(Y_test_scaled, prediction)\nprint(f\"Mean Squared Error: {mse}\")\n\nr2 = r2_score(Y_test_scaled, prediction)\nprint(f\"R²: {r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:19.310984Z","iopub.status.idle":"2025-02-02T12:57:19.311270Z","shell.execute_reply":"2025-02-02T12:57:19.311154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NeuralNetwork:\n    def __init__(self, X_train, Y_train, X_test, Y_test, LR, iterations, layer_dimensions):\n        self.X_train = X_train\n        self.Y_train = Y_train\n        self.X_test = X_test\n        self.Y_test = Y_test\n        self.LR = LR\n        self.iterations = iterations\n        self.layer_dimensions = layer_dimensions\n        self.n, self.m = self.X_train.shape\n\n        W, B = self.init_params(layer_dimensions)\n        self.W = W\n        self.B = B\n\n        self.Z = [None for i in range(len(layer_dimensions))]\n        self.Z_test = [None for i in range(len(layer_dimensions))]\n        self.A = [X_train] + [None for i in range(len(layer_dimensions) - 1)]\n        self.A_test = [X_test] + [None for i in range(len(layer_dimensions) - 1)]\n        self.dZ = [None for i in range(len(layer_dimensions))]\n        self.dW = [None for i in range(len(layer_dimensions))]\n        self.dB = [None for i in range(len(layer_dimensions))]\n\n    def init_params(self, layer_dimensions):\n        W = [None]\n        B = [None]\n    \n        for l in range(1, len(layer_dimensions)):\n            current_layer_dimension = layer_dimensions[l]\n            previous_layer_dimension = layer_dimensions[l - 1]\n            w = np.random.randn(current_layer_dimension, previous_layer_dimension) * np.sqrt(2 / previous_layer_dimension)\n            b = np.random.randn(current_layer_dimension, 1)\n            W.append(w)\n            B.append(b)\n\n        return W, B\n\n    def LeakyReLU(self, Z, alpha=0.01):\n        return np.where(Z > 0, Z, alpha * Z)\n    \n    def derivative_of_LeakyReLU(self, Z, alpha=0.01):\n        return np.where(Z > 0, 1, alpha)\n\n    def mean_squared_error(self, Y_true, Y_pred):\n        return np.mean((Y_true.reshape(-1, 1) - Y_pred.T) ** 2)\n\n    def r2_score(self, Y_true, Y_pred):\n        residual_sum_of_squares = np.sum((Y_true - Y_pred) ** 2)\n        total_sum_of_squares = np.sum((Y_true - np.mean(Y_true)) ** 2)\n        return 1 - (residual_sum_of_squares / total_sum_of_squares)\n\n    def derivative_of_mse(self, Y_true, prediction):\n        # f(Ŷ) = (Y - Ŷ)²\n        # u(Ŷ) = (Y - Ŷ)\n        # u'(Ŷ) = -1\n        # f'(Ŷ) = (u²)'•-1 = 2u•-1\n        # f'(Ŷ) = 2(Ŷ - Y)\n        return 2 * (prediction - Y_true)\n    \n    def _is_last_layer(self, layer):\n        return layer == len(self.layer_dimensions) - 1\n\n    def forward_propagation(self):\n        for layer in range(1, len(self.layer_dimensions)):\n            self.Z[layer] = self.W[layer].dot(self.A[layer - 1]) + self.B[layer]\n\n            if self._is_last_layer(layer):\n                self.A[layer] = self.Z[layer]\n            else:\n                self.A[layer] = self.LeakyReLU(self.Z[layer])\n            \n            has_nan = np.isnan(self.A[layer]).any()\n            print(f'layer {layer}: ', has_nan)\n\n    def backward_propagation(self, lambda_reg=0.01):\n        for layer in range(len(self.layer_dimensions) - 1, 0, -1):\n            if self._is_last_layer(layer):\n                self.dZ[layer] = self.derivative_of_mse(self.Y_train, self.A[layer])\n            else:\n                self.dZ[layer] = self.W[layer + 1].T.dot(self.dZ[layer + 1]) * self.derivative_of_LeakyReLU(self.Z[layer])\n\n            self.dW[layer] = 1 / self.m * self.dZ[layer].dot(self.A[layer - 1].T) + (lambda_reg / self.m) * self.W[layer]\n            self.dB[layer] = 1 / self.m * np.sum(self.dZ[layer]) \n    \n    def update_params(self):\n        for layer in range(1, len(self.layer_dimensions)):\n            self.W[layer] = self.W[layer] - self.LR * self.dW[layer]\n            self.B[layer] = self.B[layer] - self.LR * self.dB[layer]\n\n    def predict(self):\n        for layer in range(1, len(self.layer_dimensions)):\n            self.Z_test[layer] = self.W[layer].dot(self.A_test[layer - 1]) + self.B[layer]\n\n            if self._is_last_layer(layer):\n                self.A_test[layer] = self.Z_test[layer]\n            else:\n                self.A_test[layer] = self.LeakyReLU(self.Z_test[layer])\n\n        return self.A_test[-1]\n\n    def gradient_descent(self):\n        train = []\n        test = []\n        \n        for i in range(self.iterations + 1):\n            print(\"Iteration: \", i)\n            self.forward_propagation()\n            self.backward_propagation()\n            self.update_params()\n\n            training_loss = self.mean_squared_error(self.Y_train, self.A[-1])\n            training_r2 = self.r2_score(self.Y_train, self.A[-1])\n            test_predictions = self.predict()\n            test_loss = self.mean_squared_error(self.Y_test, test_predictions)\n            test_r2 = self.r2_score(self.Y_test, test_predictions)\n    \n            train.append(training_loss)\n            test.append(test_loss)\n    \n            if i % 10 == 0:\n                print('- training_loss: ', training_loss)\n                print('- training_r2: ', training_r2)\n                print('- test_loss: ', test_loss)\n                print('- test_r2: ', test_r2)\n    \n        return train, test\n\nLR = 0.1\nITERATIONS = 100\nlayer_dimensions = [18, 18, 18, 18, 18, 18, 1]\n\nnn = NeuralNetwork(X_training_scaled.T, Y_training_scaled.T, X_test_scaled.T, Y_test_scaled.T, LR, ITERATIONS, layer_dimensions)\ntrain, test = nn.gradient_descent()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:57:45.628748Z","iopub.execute_input":"2025-02-02T12:57:45.629142Z","iopub.status.idle":"2025-02-02T12:58:21.728876Z","shell.execute_reply.started":"2025-02-02T12:57:45.629111Z","shell.execute_reply":"2025-02-02T12:58:21.727858Z"}},"outputs":[{"name":"stdout","text":"Iteration:  0\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.5281260099465377\n- training_r2:  -0.5281260099465377\n- test_loss:  9.699960684725378\n- test_r2:  -8.060421231891684\nIteration:  1\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  2\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  3\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  4\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  5\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  6\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  7\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  8\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  9\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  10\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.0379725414848473\n- training_r2:  -0.0379725414848473\n- test_loss:  1.0940843288369364\n- test_r2:  -0.0219489753277069\nIteration:  11\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  12\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  13\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  14\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  15\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  16\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  17\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  18\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  19\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  20\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.0012336294796575\n- training_r2:  -0.0012336294796575498\n- test_loss:  1.0709340002999848\n- test_r2:  -0.0003249981778032929\nIteration:  21\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  22\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  23\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  24\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  25\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  26\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  27\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  28\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  29\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  30\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.0004980476483185\n- training_r2:  -0.0004980476483185114\n- test_loss:  1.0706143991531727\n- test_r2:  -2.6469028002518158e-05\nIteration:  31\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  32\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  33\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  34\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  35\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  36\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  37\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  38\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  39\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  40\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.0003067377412767\n- training_r2:  -0.00030673774127665254\n- test_loss:  1.0706036285534206\n- test_r2:  -1.6408557259817158e-05\nIteration:  41\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  42\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  43\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  44\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  45\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  46\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  47\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  48\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  49\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  50\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.000184018942441\n- training_r2:  -0.0001840189424409111\n- test_loss:  1.0706025835180648\n- test_r2:  -1.5432423352113034e-05\nIteration:  51\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  52\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  53\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  54\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  55\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  56\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  57\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  58\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  59\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  60\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.000113203682353\n- training_r2:  -0.00011320368235301359\n- test_loss:  1.0706023376076046\n- test_r2:  -1.5202726302065983e-05\nIteration:  61\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  62\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  63\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  64\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  65\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  66\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  67\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  68\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  69\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  70\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.000071171005303\n- training_r2:  -7.117100530296128e-05\n- test_loss:  1.0706021980478744\n- test_r2:  -1.5072368047652773e-05\nIteration:  71\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  72\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  73\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  74\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  75\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  76\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  77\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  78\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  79\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  80\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.0000456851981627\n- training_r2:  -4.568519816272598e-05\n- test_loss:  1.070602089438893\n- test_r2:  -1.497091989222099e-05\nIteration:  81\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  82\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  83\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  84\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  85\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  86\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  87\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  88\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  89\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  90\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.0000346182494584\n- training_r2:  -3.4618249458384653e-05\n- test_loss:  1.0706020143987585\n- test_r2:  -1.490082731647746e-05\nIteration:  91\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  92\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  93\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  94\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  95\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  96\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  97\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  98\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  99\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\nIteration:  100\nlayer 1:  False\nlayer 2:  False\nlayer 3:  False\nlayer 4:  False\nlayer 5:  False\nlayer 6:  False\n- training_loss:  1.0000267116122976\n- training_r2:  -2.6711612297614096e-05\n- test_loss:  1.0706019500757236\n- test_r2:  -1.4840745238942077e-05\n","output_type":"stream"}],"execution_count":39}]}