{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30035,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Network from scratch","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndata = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T21:49:48.744521Z","iopub.status.idle":"2024-11-16T21:49:48.745052Z","shell.execute_reply":"2024-11-16T21:49:48.744796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T21:49:48.746162Z","iopub.status.idle":"2024-11-16T21:49:48.746714Z","shell.execute_reply":"2024-11-16T21:49:48.746415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = np.array(data)\nm, n = data.shape\nm, n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T21:49:48.748106Z","iopub.status.idle":"2024-11-16T21:49:48.748665Z","shell.execute_reply":"2024-11-16T21:49:48.748350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"number_of_tests = int(m * 0.3)\n\nnp.random.shuffle(data) # shuffle before splitting into test and training sets\n\ntest_data = data[0:number_of_tests].T\nY_test = test_data[0]\nX_test = test_data[1:n]\nX_test = X_test / 255.0\n\ntrain_data = data[number_of_tests:m].T\nY_train = train_data[0]\nX_train = train_data[1:n]\nX_train = X_train / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T21:49:48.749925Z","iopub.status.idle":"2024-11-16T21:49:48.750455Z","shell.execute_reply":"2024-11-16T21:49:48.750175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # def init_params():\n# #     W1 = np.random.randn(100, 784) * 0.01\n# #     B1 = np.zeros((100, 1))\n# #     W2 = np.random.randn(100, 100) * 0.01\n# #     B2 = np.zeros((100, 1))\n# #     W3 = np.random.randn(10, 100) * 0.01\n# #     B3 = np.zeros((10, 1))\n# #     return W1, B1, W2, B2, W3, B3\n\n# def init_params():\n#     W1 = np.random.randn(10, 784) * np.sqrt(2 / 784)\n#     B1 = np.zeros((10, 1))\n#     W2 = np.random.randn(10, 10) * np.sqrt(2 / 10) \n#     B2 = np.zeros((10, 1))\n#     W3 = np.random.randn(10, 10) * np.sqrt(2 / 10) \n#     B3 = np.zeros((10, 1))\n#     return W1, B1, W2, B2, W3, B3\n\n# def ReLU(Z):\n#     return np.maximum(Z, 0)\n\n# def LeakyReLU(Z, alpha=0.01):\n#     return np.where(Z > 0, Z, alpha * Z)\n\n# def derivative_of_LeakyReLU(Z, alpha=0.01):\n#     return np.where(Z > 0, 1, alpha)\n\n# # def softmax(Z):\n# #     return np.exp(Z) / sum(np.exp(Z))\n\n# def softmax(Z):\n#     expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n#     return expZ / np.sum(expZ, axis=0, keepdims=True)\n    \n# # def forward_propagation(W1, B1, W2, B2, W3, B3, X):\n# #     Z1 = W1.dot(X) + B1\n# #     A1 = LeakyReLU(Z1)\n# #     Z2 = W2.dot(A1) + B2\n# #     A2 = LeakyReLU(Z2)\n# #     Z3 = W3.dot(A2) + B3\n# #     A3 = softmax(Z3)\n# #     return Z1, A1, Z2, A2, Z3, A3\n\n# def forward_propagation(W1, B1, W2, B2, W3, B3, X):\n#     Z1 = W1.dot(X) + B1\n#     A1 = ReLU(Z1)\n#     Z2 = W2.dot(A1) + B2\n#     A2 = ReLU(Z2)\n#     Z3 = W3.dot(A2) + B3\n#     A3 = softmax(Z3)\n#     return Z1, A1, Z2, A2, Z3, A3\n\n# def derivative_of_ReLU(Z):\n#     return Z > 0\n\n# def one_hot(Y):\n#     one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n#     one_hot_Y[np.arange(Y.size), Y] = 1\n#     return one_hot_Y.T\n\n# def backward_propagation(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y):\n#     dZ3 = A3 - Y\n#     dW3 = 1 / m * dZ3.dot(A2.T)\n#     dB3 = 1 / m * np.sum(dZ3)\n    \n#     dZ2 = W3.T.dot(dZ3) * derivative_of_ReLU(Z2)\n#     dW2 = 1 / m * dZ2.dot(A1.T)\n#     dB2 = 1 / m * np.sum(dZ2)\n    \n#     dZ1 = W2.T.dot(dZ2) * derivative_of_ReLU(Z1)\n#     dW1 = 1 / m * dZ1.dot(X.T)\n#     dB1 = 1 / m * np.sum(dZ1)\n    \n#     return dW1, dB1, dW2, dB2, dW3, dB3\n\n# # def backward_propagation(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y, lambda_reg=0.01):\n# #     dZ3 = A3 - Y\n# #     dW3 = 1 / m * dZ3.dot(A2.T) + (lambda_reg / m) * W3\n# #     dB3 = 1 / m * np.sum(dZ3)\n    \n# #     dZ2 = W3.T.dot(dZ3) * derivative_of_LeakyReLU(Z2)\n# #     dW2 = 1 / m * dZ2.dot(A1.T) + (lambda_reg / m) * W2\n# #     dB2 = 1 / m * np.sum(dZ2)\n    \n# #     dZ1 = W2.T.dot(dZ2) * derivative_of_LeakyReLU(Z1)\n# #     dW1 = 1 / m * dZ1.dot(X.T) + (lambda_reg / m) * W1\n# #     dB1 = 1 / m * np.sum(dZ1)\n    \n# #     return dW1, dB1, dW2, dB2, dW3, dB3\n\n# def update_params(W1, B1, W2, B2, W3, B3, dW1, dB1, dW2, dB2, dW3, dB3, LR):\n#     W1 = W1 - LR * dW1\n#     B1 = B1 - LR * dB1    \n#     W2 = W2 - LR * dW2  \n#     B2 = B2 - LR * dB2\n#     W3 = W3 - LR * dW3  \n#     B3 = B3 - LR * dB3\n#     return W1, B1, W2, B2, W3, B3\n\ndef init_params():\n    W1 = np.random.rand(100, 784) * 0.01\n    B1 = np.zeros((100, 1))\n    W2 = np.random.rand(10, 100) * 0.01\n    B2 = np.zeros((10, 1))\n    return W1, B1, W2, B2\n\ndef ReLU(Z):\n    return np.maximum(Z, 0)\n\ndef softmax(Z):\n    return np.exp(Z) / sum(np.exp(Z))\n    \ndef forward_propagation(W1, B1, W2, B2, X):\n    Z1 = W1.dot(X) + B1\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + B2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef derivative_of_ReLU(Z):\n    return Z > 0\n\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    return one_hot_Y.T\n\ndef backward_propagation(Z1, A1, Z2, A2, W1, W2, X, Y):\n    dZ2 = A2 - Y\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    dB2 = 1 / m * np.sum(dZ2)\n    dZ1 = W2.T.dot(dZ2) * derivative_of_ReLU(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    dB1 = 1 / m * np.sum(dZ1)\n    return dW1, dB1, dW2, dB2\n\ndef update_params(W1, B1, W2, B2, dW1, dB1, dW2, dB2, LR):\n    W1 = W1 - LR * dW1\n    B1 = B1 - LR * dB1    \n    W2 = W2 - LR * dW2  \n    B2 = B2 - LR * dB2    \n    return W1, B1, W2, B2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def get_predictions(A2):\n#     return np.argmax(A2, 0)\n\n# def get_accuracy(predictions, Y):\n#     return np.sum(predictions == Y) / Y.size\n\n# def gradient_descent(X, Y, LR, iterations):\n#     W1, B1, W2, B2, W3, B3 = init_params()\n#     one_hot_Y = one_hot(Y)\n    \n#     for i in range(iterations + 1):\n#         Z1, A1, Z2, A2, Z3, A3 = forward_propagation(W1, B1, W2, B2, W3, B3, X)\n#         dW1, dB1, dW2, dB2, dW3, dB3 = backward_propagation(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, one_hot_Y)\n#         W1, B1, W2, B2, W3, B3 = update_params(W1, B1, W2, B2, W3, B3, dW1, dB1, dW2, dB2, dW3, dB3, LR)\n#         if i % 10 == 0:\n#             print(\"Iteration: \", i)\n#             predictions = get_predictions(A3)\n#             print(get_accuracy(predictions, Y))\n#     return W1, B1, W2, B2, W3, B3\n\ndef get_predictions(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, LR, iterations):\n    W1, B1, W2, B2 = init_params()\n    one_hot_Y = one_hot(Y)\n    \n    for i in range(iterations + 1):\n        Z1, A1, Z2, A2 = forward_propagation(W1, B1, W2, B2, X)\n        dW1, db1, dW2, db2 = backward_propagation(Z1, A1, Z2, A2, W1, W2, X, one_hot_Y)\n        W1, B1, W2, B2 = update_params(W1, B1, W2, B2, dW1, db1, dW2, db2, LR)\n        if i % 10 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, B1, W2, B2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# W1, B1, W2, B2, W3, B3 = gradient_descent(X_train, Y_train, 0.0001, 500)\nW1, B1, W2, B2 = gradient_descent(X_train, Y_train, 0.1, 500)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"~85% accuracy on training set.","metadata":{}},{"cell_type":"code","source":"def make_predictions(X, W1, B1, W2, B2):\n    _, _, _, A2 = forward_propagation(W1, B1, W2, B2, X)\n    predictions = get_predictions(A2)\n    return predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's look at a couple of examples:","metadata":{}},{"cell_type":"code","source":"test_predictions = make_predictions(X_test, W1, B1, W2, B2)\nget_accuracy(test_predictions, Y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Still 84% accuracy, so our model generalized from the training data pretty well.","metadata":{}},{"cell_type":"code","source":"def init_params():\n    W1 = np.random.randn(10, 784) * np.sqrt(2 / 784)\n    B1 = np.zeros((10, 1))\n    W2 = np.random.randn(10, 10) * np.sqrt(2 / 10) \n    B2 = np.zeros((10, 1))\n    W3 = np.random.randn(10, 10) * np.sqrt(2 / 10) \n    B3 = np.zeros((10, 1))\n    return W1, B1, W2, B2, W3, B3\n\ndef ReLU(Z):\n    return np.maximum(Z, 0)\n\ndef LeakyReLU(Z, alpha=0.01):\n    return np.where(Z > 0, Z, alpha * Z)\n\ndef derivative_of_LeakyReLU(Z, alpha=0.01):\n    return np.where(Z > 0, 1, alpha)\n\ndef softmax(Z):\n    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n    return expZ / np.sum(expZ, axis=0, keepdims=True)\n    \ndef forward_propagation(W1, B1, W2, B2, W3, B3, X):\n    Z1 = W1.dot(X) + B1\n    A1 = LeakyReLU(Z1)\n    Z2 = W2.dot(A1) + B2\n    A2 = LeakyReLU(Z2)\n    Z3 = W3.dot(A2) + B3\n    A3 = softmax(Z3)\n    return Z1, A1, Z2, A2, Z3, A3\n\ndef derivative_of_ReLU(Z):\n    return Z > 0\n\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    return one_hot_Y.T\n\ndef backward_propagation(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y, lambda_reg=0.01):\n    dZ3 = A3 - Y\n    dW3 = 1 / m * dZ3.dot(A2.T) + (lambda_reg / m) * W3\n    dB3 = 1 / m * np.sum(dZ3)\n    \n    dZ2 = W3.T.dot(dZ3) * derivative_of_LeakyReLU(Z2)\n    dW2 = 1 / m * dZ2.dot(A1.T) + (lambda_reg / m) * W2\n    dB2 = 1 / m * np.sum(dZ2)\n    \n    dZ1 = W2.T.dot(dZ2) * derivative_of_LeakyReLU(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T) + (lambda_reg / m) * W1\n    dB1 = 1 / m * np.sum(dZ1)\n    \n    return dW1, dB1, dW2, dB2, dW3, dB3\n\ndef update_params(W1, B1, W2, B2, W3, B3, dW1, dB1, dW2, dB2, dW3, dB3, LR):\n    W1 = W1 - LR * dW1\n    B1 = B1 - LR * dB1    \n    W2 = W2 - LR * dW2  \n    B2 = B2 - LR * dB2\n    W3 = W3 - LR * dW3  \n    B3 = B3 - LR * dB3\n    return W1, B1, W2, B2, W3, B3\n\ndef get_predictions(A):\n    return np.argmax(A, 0)\n\ndef get_accuracy(predictions, Y):\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, LR, iterations):\n    W1, B1, W2, B2, W3, B3 = init_params()\n    one_hot_Y = one_hot(Y)\n    \n    for i in range(iterations + 1):\n        Z1, A1, Z2, A2, Z3, A3 = forward_propagation(W1, B1, W2, B2, W3, B3, X)\n        dW1, dB1, dW2, dB2, dW3, dB3 = backward_propagation(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, one_hot_Y)\n        W1, B1, W2, B2, W3, B3 = update_params(W1, B1, W2, B2, W3, B3, dW1, dB1, dW2, dB2, dW3, dB3, LR)\n        if i % 10 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A3)\n            print(get_accuracy(predictions, Y))\n    return W1, B1, W2, B2, W3, B3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T21:49:50.963114Z","iopub.execute_input":"2024-11-16T21:49:50.963506Z","iopub.status.idle":"2024-11-16T21:49:51.008189Z","shell.execute_reply.started":"2024-11-16T21:49:50.963473Z","shell.execute_reply":"2024-11-16T21:49:51.006888Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"W1, B1, W2, B2, W3, B3 = gradient_descent(X_train, Y_train, 0.1, 1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T22:14:03.220019Z","iopub.execute_input":"2024-11-16T22:14:03.221143Z","iopub.status.idle":"2024-11-16T22:15:28.211222Z","shell.execute_reply.started":"2024-11-16T22:14:03.221072Z","shell.execute_reply":"2024-11-16T22:15:28.210072Z"}},"outputs":[{"name":"stdout","text":"Iteration:  0\n0.065\nIteration:  10\n0.15404761904761904\nIteration:  20\n0.25666666666666665\nIteration:  30\n0.332108843537415\nIteration:  40\n0.48741496598639455\nIteration:  50\n0.5715986394557823\nIteration:  60\n0.606156462585034\nIteration:  70\n0.6463265306122449\nIteration:  80\n0.697687074829932\nIteration:  90\n0.7402380952380953\nIteration:  100\n0.7624149659863946\nIteration:  110\n0.7777891156462585\nIteration:  120\n0.79\nIteration:  130\n0.8008503401360544\nIteration:  140\n0.8091496598639456\nIteration:  150\n0.8182993197278912\nIteration:  160\n0.8249319727891157\nIteration:  170\n0.8310884353741497\nIteration:  180\n0.8364965986394558\nIteration:  190\n0.8407142857142857\nIteration:  200\n0.8447959183673469\nIteration:  210\n0.8485034013605443\nIteration:  220\n0.8517687074829932\nIteration:  230\n0.8549659863945578\nIteration:  240\n0.8573469387755102\nIteration:  250\n0.8600680272108844\nIteration:  260\n0.8625850340136054\nIteration:  270\n0.865204081632653\nIteration:  280\n0.8680952380952381\nIteration:  290\n0.8697619047619047\nIteration:  300\n0.8714965986394558\nIteration:  310\n0.8738095238095238\nIteration:  320\n0.8756802721088436\nIteration:  330\n0.8774149659863946\nIteration:  340\n0.8790136054421769\nIteration:  350\n0.8804081632653061\nIteration:  360\n0.8819727891156462\nIteration:  370\n0.8834353741496599\nIteration:  380\n0.8851700680272109\nIteration:  390\n0.8865986394557823\nIteration:  400\n0.8879591836734694\nIteration:  410\n0.8889795918367347\nIteration:  420\n0.8903401360544217\nIteration:  430\n0.8919387755102041\nIteration:  440\n0.8927210884353741\nIteration:  450\n0.8934353741496599\nIteration:  460\n0.8944557823129252\nIteration:  470\n0.8952380952380953\nIteration:  480\n0.8963265306122449\nIteration:  490\n0.8973809523809524\nIteration:  500\n0.8981632653061224\nIteration:  510\n0.8990816326530612\nIteration:  520\n0.9\nIteration:  530\n0.9008843537414966\nIteration:  540\n0.9016666666666666\nIteration:  550\n0.9025510204081633\nIteration:  560\n0.9034013605442177\nIteration:  570\n0.9042517006802722\nIteration:  580\n0.9052380952380953\nIteration:  590\n0.9057482993197279\nIteration:  600\n0.9062244897959184\nIteration:  610\n0.9069047619047619\nIteration:  620\n0.9070748299319727\nIteration:  630\n0.9076190476190477\nIteration:  640\n0.9082653061224489\nIteration:  650\n0.9087414965986395\nIteration:  660\n0.9094557823129251\nIteration:  670\n0.9097619047619048\nIteration:  680\n0.9101700680272109\nIteration:  690\n0.9107142857142857\nIteration:  700\n0.9110544217687074\nIteration:  710\n0.9114965986394558\nIteration:  720\n0.9119727891156463\nIteration:  730\n0.9124149659863946\nIteration:  740\n0.9128571428571428\nIteration:  750\n0.9132312925170069\nIteration:  760\n0.9136394557823129\nIteration:  770\n0.914047619047619\nIteration:  780\n0.9144557823129251\nIteration:  790\n0.9151020408163265\nIteration:  800\n0.915374149659864\nIteration:  810\n0.9159863945578232\nIteration:  820\n0.9163605442176871\nIteration:  830\n0.9165986394557823\nIteration:  840\n0.9170068027210885\nIteration:  850\n0.9174829931972789\nIteration:  860\n0.9176530612244898\nIteration:  870\n0.9177891156462585\nIteration:  880\n0.9183333333333333\nIteration:  890\n0.918469387755102\nIteration:  900\n0.9187755102040817\nIteration:  910\n0.9189795918367347\nIteration:  920\n0.919421768707483\nIteration:  930\n0.9197278911564626\nIteration:  940\n0.9198979591836735\nIteration:  950\n0.9201700680272109\nIteration:  960\n0.920374149659864\nIteration:  970\n0.9207142857142857\nIteration:  980\n0.9208503401360544\nIteration:  990\n0.9208843537414966\nIteration:  1000\n0.9212925170068027\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"def make_predictions(X, W1, B1, W2, B2, W3, B3):\n    _, _, _, _, _, A3 = forward_propagation(W1, B1, W2, B2, W3, B3, X)\n    predictions = get_predictions(A3)\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T21:50:36.492924Z","iopub.execute_input":"2024-11-16T21:50:36.493466Z","iopub.status.idle":"2024-11-16T21:50:36.505866Z","shell.execute_reply.started":"2024-11-16T21:50:36.493412Z","shell.execute_reply":"2024-11-16T21:50:36.504833Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"test_predictions = make_predictions(X_test, W1, B1, W2, B2, W3, B3)\nget_accuracy(test_predictions, Y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T22:15:28.213825Z","iopub.execute_input":"2024-11-16T22:15:28.214193Z","iopub.status.idle":"2024-11-16T22:15:28.252140Z","shell.execute_reply.started":"2024-11-16T22:15:28.214158Z","shell.execute_reply":"2024-11-16T22:15:28.250556Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"0.907063492063492"},"metadata":{}}],"execution_count":42}]}